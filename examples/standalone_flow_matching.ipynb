{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn, Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat\n",
    "\n",
    "class Flow(nn.Module):\n",
    "    def __init__(self, dim: int = 2, h: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim + 1, h), nn.ELU(),\n",
    "            nn.Linear(h, h), nn.ELU(),\n",
    "            nn.Linear(h, h), nn.ELU(),\n",
    "            nn.Linear(h, dim))\n",
    "    \n",
    "    def forward(self, t: Tensor, x_t: Tensor) -> Tensor:\n",
    "        return self.net(torch.cat((t, x_t), -1))\n",
    "    \n",
    "    def step(self, x_t: Tensor, t_start: Tensor, t_end: Tensor) -> Tensor:\n",
    "        t_start = t_start.view(1, 1).expand(x_t.shape[0], 1)\n",
    "        \n",
    "        return x_t + (t_end - t_start) * self(t=t_start + (t_end - t_start) / 2, x_t= x_t + self(x_t=x_t, t=t_start) * (t_end - t_start) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = Flow()\n",
    "\n",
    "optimizer = torch.optim.Adam(flow.parameters(), 1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for _ in range(10000):\n",
    "    x_1 = Tensor(make_moons(256, noise=0.15)[0])\n",
    "    x_0 = torch.randn_like(x_1)\n",
    "    t = torch.rand(len(x_1), 1)\n",
    "    \n",
    "    x_t = (1 - t) * x_0 + t * x_1\n",
    "    dx_t = x_1 - x_0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_fn(flow(t=t, x_t=x_t), dx_t).backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(300, 2)\n",
    "n_steps = 8\n",
    "fig, axes = plt.subplots(1, n_steps + 1, figsize=(30, 4), sharex=True, sharey=True)\n",
    "time_steps = torch.linspace(0, 1.0, n_steps + 1)\n",
    "\n",
    "axes[0].scatter(x.detach()[:, 0], x.detach()[:, 1], s=10)\n",
    "axes[0].set_title(f't = {time_steps[0]:.2f}')\n",
    "axes[0].set_xlim(-3.0, 3.0)\n",
    "axes[0].set_ylim(-3.0, 3.0)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    x = flow.step(x_t=x, t_start=time_steps[i], t_end=time_steps[i + 1])\n",
    "    axes[i + 1].scatter(x.detach()[:, 0], x.detach()[:, 1], s=10)\n",
    "    axes[i + 1].set_title(f't = {time_steps[i + 1]:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Flow Matching\n",
    "We now train a simple conditional model using class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn, Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat\n",
    "\n",
    "class Flow(nn.Module):\n",
    "    def __init__(self, dim: int = 2, h: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim + 2, h), nn.ELU(),\n",
    "            nn.Linear(h, h), nn.ELU(),\n",
    "            nn.Linear(h, h), nn.ELU(),\n",
    "            nn.Linear(h, dim))\n",
    "    \n",
    "    def forward(self, t: Tensor, c: Tensor, x_t: Tensor ) -> Tensor:\n",
    "        return self.net(torch.cat((t, c, x_t), -1))\n",
    "    \n",
    "    def step(self, x_t: Tensor, t_start: Tensor, t_end: Tensor, c: Tensor) -> Tensor:\n",
    "        t_start = t_start.view(1, 1).expand(x_t.shape[0], 1)\n",
    "        \n",
    "        return x_t + (t_end - t_start) * self(t=t_start + (t_end - t_start) / 2, c = c, x_t= x_t + self(c = c, x_t=x_t, t=t_start) * (t_end - t_start) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = Flow()\n",
    "\n",
    "optimizer = torch.optim.Adam(flow.parameters(), 1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for _ in range(10000):\n",
    "    x_1, c = make_moons(256, noise=0.15)\n",
    "    x_1 = Tensor(x_1)\n",
    "    c = Tensor(c)\n",
    "    c = c.view(-1, 1)\n",
    "    \n",
    "    x_0 = torch.randn_like(x_1)\n",
    "    t = torch.rand(len(x_1), 1)\n",
    "    \n",
    "    x_t = (1 - t) * x_0 + t * x_1\n",
    "    dx_t = x_1 - x_0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_fn(flow(t=t, x_t=x_t, c=c), dx_t).backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- evaluation / visualisation section --------------------------\n",
    "n_samples = 256                    \n",
    "\n",
    "sigma = 1.0\n",
    "x      = torch.randn(n_samples, 2) * sigma     # (n_samples, 2)\n",
    "\n",
    "# if you just want random labels –– otherwise load real labels here\n",
    "c_eval = torch.randint(0, 2, (n_samples, 1), dtype=torch.float32)  # (n_samples, 1)\n",
    "\n",
    "# colours for the scatter (same length as x)\n",
    "colors  = ['blue' if lbl == 0 else 'orange' for lbl in c_eval.squeeze().tolist()]\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "n_steps      = 100\n",
    "plot_every   = 20\n",
    "plot_indices = list(range(0, n_steps + 1, plot_every))\n",
    "if plot_indices[-1] != n_steps:\n",
    "    plot_indices.append(n_steps)\n",
    "\n",
    "fig, axes   = plt.subplots(1, len(plot_indices), figsize=(4 * len(plot_indices), 4),\n",
    "                           sharex=True, sharey=True)\n",
    "time_steps  = torch.linspace(0, 1.0, n_steps + 1)\n",
    "\n",
    "# initial frame\n",
    "axes[0].scatter(x[:, 0], x[:, 1], s=10, c=colors)\n",
    "axes[0].set_title(f't = {time_steps[0]:.2f}')\n",
    "axes[0].set_xlim(-3.0, 3.0)\n",
    "axes[0].set_ylim(-3.0, 3.0)\n",
    "\n",
    "plot_count = 0\n",
    "with torch.no_grad():                         # no gradients while sampling\n",
    "    for i in range(n_steps):\n",
    "        x = flow.step(x_t=x,\n",
    "                      t_start=time_steps[i],\n",
    "                      t_end=time_steps[i + 1],\n",
    "                      c=c_eval)               # 2️⃣ use the same‑sized label tensor\n",
    "        if (i + 1) in plot_indices:\n",
    "            plot_count += 1\n",
    "            axes[plot_count].scatter(x[:, 0], x[:, 1], s=10, c=colors)\n",
    "            axes[plot_count].set_title(f't = {time_steps[i + 1]:.2f}')\n",
    "            axes[plot_count].set_xlim(-3.0, 3.0)\n",
    "            axes[plot_count].set_ylim(-3.0, 3.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow_matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
